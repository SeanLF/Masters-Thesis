% Chapter 1

\chapter{Introduction\label{chapter:introduction}} % Main chapter title

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

Analytical models have been studied and developed by researchers, with increasing intensity over the last two decades, to intelligently and computationally extract knowledge from data \cite{bifet2009data}. The quantity and size of data sets have grown exponentially over that time, requiring new algorithms to respect new constraints. Data streams are a latest data format that researchers are developing techniques for, and, are characterised by velocious and continuous flows of data \cite{krempl2014open}. This format requires algorithms to update their models to adapt to potential changes to the underlying concepts that the stream represent over time. Techniques have been developed to explicitly detect these changes to help models to adapt more quickly in order to stay relevant and accurate. Researchers have shown a continuing interest in the development of algorithms to extract knowledge from evolving streams \cite{gama2010knowledge, gama2014survey, ghesmoune2016state, KRAWCZYK2017132, krempl2014open, silva2013data, widmer1996learning}. Another current topic of interest is the development of ensembles, which are defined as an amalgamation of any number of analytical models. Ensembles are considered as one of the most promising research directions nowadays \cite{jain2000statistical, KRAWCZYK2017132, oza2008classifier, polikar2006ensemble, rokach2009taxonomy, wozniak2014survey}.

An abundance of algorithms have been proposed in the literature to use ensembles to learn from evolving streams, that do so either online (read instance-by-instance) or in chunks. These algorithms typically learn in batches by training new classifiers on each incoming chunk, and either use some weighting scheme or replacement strategy that relies on the use of timely and correctly labelled data. Alternatively, algorithms can learn online by using sliding windows of a variable, or fixed, size to summarise the data and appropriately update their models from them.

Drift detection methods have also mainly relied on the use of labelled data by detecting changes in the accuracy of a classifier over time. \textbf{On the other hand} drift detectors that are unsupervised, mostly rely on statistical tests \cite{dries2009adaptive, friedman1979multivariate, sheskin2003handbook, sobolewski2013concept}.

%state the purpose of the study. 
%allow readers to understand the background to the study, without needing to consult the literature themselves.
%To describe the historical development of the topic.
%To provide a context for the later discussion of the results.

%----------------------------------------------------------------------------------------

\section{Motivation}
A major issue with supervised and semi-supervised learning of data streams is late-arriving or missing class labels\textbf{. For example, a bank cannot know if a loan will default before several months have passed, if not years}. Assuming that correctly labelled data will always be available and timely is unreasonable, and, as such, supervised methods are not usually applicable in the real world. Therefore, real-world problems usually require the use of semi-supervised or unsupervised learning techniques. To the best of our knowledge, no research has been conducted to develop semi-supervised techniques to learn from evolving streams without clustering unlabelled data, which is computationally expensive \cite{krempl2014open}. Furthermore, semi-supervised drift detecting algorithms for streams \textbf{are more} applicable to real-world applications, but only one article has been published as of yet \cite{haque2015sand}.
%----------------------------------------------------------------------------------------

\section{Thesis Objective}
The purpose of this thesis is to contribute to narrow the gap in research as it pertains to semi-supervised learning of evolving streams, without clustering unlabelled data.

Therefore, we combined our contributions to introduce a framework, \textbf{LESS-TWE} (Learning from Evolving Streams via Self-Training Windowing Ensembles). Our framework employs self-training, a novel windowing technique exclusive to ensembles, a new weighted soft voting strategy, and an extension of \textbf{Fast Hoeffding Drift Detection Method for evolving Streams (FHDDMS)} to work without labelled data.

Research has not yet been conducted to study how selective self-training\textbf{, a semi-supervised learning (SSL) algorithm,} performs when removing the selective heuristic, thereby predicting labels for unlabelled data then training on it. \textbf{Selective self-training learns from originally unlabelled data that it annotates with labels it predicts for which it has high confidence in.} This presents one of the objectives for this thesis.

By introducing the novel windowing technique, we aim to investigate if savings relating to the execution time can be achieved while maintaining comparable predictive accuracy. Additionally, we can observe if delaying the training of some the classifiers in the ensemble affects concept drift detection.

Finally, we introduce a weighted soft voting scheme for ensembles, in conjunction with our extension to FHDDMS to detect drifts without relying on labelled data.

%----------------------------------------------------------------------------------------

\section{Thesis Organisation}
\textbf{This thesis is organised as follows.}

In chapter \ref{chapter:background_work} we review the background work surrounding data stream mining, ensemble learners, and concept drift detection.

Following the review, we will present the contributions made by this thesis to the literature in chapter \ref{chapter:contributions}.

As stated above, this will cover improvements to an existing concept drift algorithm to reduce its dependency on ground truth, improvements to a simple voting classifier to also further reduce its dependency on ground truth and finally a novel windowing technique that combines sliding and tumbling techniques.

In chapter \ref{chapter:experimental_design}, we will present the experimental design for testing our contributions and present the results and discuss them in chapter \ref{chapter:evaluation_discussion}.

Finally we conclude in chapter \ref{chapter:conclusion}.