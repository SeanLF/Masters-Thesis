%----------------------------------------------------------------------------------------
\section{Window size}
\subsection{Without drift detection}
This experiment determines whether or not there is a significant statistical difference in the execution time and prediction accuracy of our voting ensemble classifier, when changing only the chunk and therefore the window sizes. In order to do so, we ran multiple simulations of prequential evaluation with varying combinations of parameters, while always making sure that each set only had different values of chunk and window sizes.
For example, one such set would be $[Hybrid,avg\_w\_probability,100\%gt]$ while another would be $[Sliding,avg\_w\_probability,100\%gt]$, etc.

\subsubsection{$\kappa_t$}
For this measure, some sets of parameters obtained a p value below the 0.05 threshold for the Friedman test and thus rejected the null hypothesis that all window sizes would perform identically in regard to $\kappa_t$. However, the post-hoc Nemenyi test confirmed the null hypothesis as all pair of window sizes failed to obtain a p value below the 0.05 threshold.

This results seems to indicate that the window and chunk size used when not actively detecting drifts has no effect on the prediction accuracy of our voting classifier.

\subsubsection{Execution time}
For this measure, the Friedman test showed a significant statistical difference with all p values below 0.001, thus rejecting the null hypothesis that, unsurprisingly, all window sizes would result in the same execution time. The post-hoc Nemenyi test showed that there was a significant statistical difference between the execution times for chunk sizes 5 and 100. This was supported by a p value of 0.05.

This results seems to indicate that there are significant time savings when using a chunk size of 100 over a chunk size of 5.

We can therefor say that this experiment seems to indicate that there is no statistically significant reason to pick any chunk size over another for improved prediction accuracy, but time savings can be had by choosing a chunk size of 100. It would seem that choosing a larger window size leads to time savings without affecting prediction accuracy.

\subsection{With drift detection}

\subsubsection{$\kappa_t$}
Only one out of 330 combinations of parameters resulted in results showing a statistical significance. It indicated that a chunk size of 25 allows better prediction accuracy than a chunk size of 100. Given that less than 1\% of parameter combinations showed a statistical significance, we are led to believe that the window size does not have an effect on prediction accuracy.

\subsubsection{Execution time}
87\% of parameter combinations show a statistical difference. 82\% of which indicate that a chunk size of 100 takes less time than a chunk size of 25 (100 ranked first, while 25 ranked last), and 18\% of which show that a chunk size of 75 takes less time than 25 (75 ranked first, 25 last).
Given that a large portion of parameter combinations show a statistical significance and reject the null hypothesis, we are led to believe that setting a larger window size allows us to save time. This is expected as there are fewer loop iterations while training and testing, while also allowing the classifiers to learn from more instances at once. The downside is that it prevents us from using small window sizes when detecting drifts, unless we divide the drift detectors content over its window size and iterate that way, which is left as future work.

\section{Voting type}
\subsection{Without drift detection}
\subsubsection{$\kappa_t$}
The Friedman test confirmed the null hypothesis indicating that there is no statistical significance in the prediction accuracy when using varying voting types. This indicates unfortunately that we cannot say that our proposed voting techniques outperform probability voting or majority voting. However, by looking at the rankings, we can see that $probability > w\_avg\_probability > avg\_w\_probability > boolean$. Furthermore, looking at the raw values, we can observe that...

\subsubsection{Execution time}
The p values for all sets of parameters output from the Friedman test indicate a very significant statistical significance with values below 0.001 thus rejecting the null hypothesis. This would indicate that the voting type has a significant effect on the execution time. The post-hoc Nemenyi test supports this with a p value of 0.05 for $avg\_w\_probability$ and $probability$ voting.
The rankings show that $probability > w\_avg\_probability > boolean > avg\_w\_probability$ in terms of execution time.

\subsection{With drift detection}

\subsubsection{$\kappa_t$}
As before, the Friedman test confirmed the null hypothesis for all combinations of parameters, indicating that the voting technique does not influence the prediction accuracy of our voting ensemble. This result is partially disappointing due to the fact that our proposed voting techniques does not outperform existing ones; however, it does not indicate that our proposed voting techniques performs worse than existing ones.

\subsubsection{Execution time}
For 43\% of the parameter combinations, the p-values from the post-hoc Nemenyi test are all below 0.001, thus indicating a very high statistical significance in the results and thus rejecting the null hypothesis that all voting types lead to the same execution time. For 92\% of the parameters leading to statistically significant results, we can see that probability voting (ranked first) takes less time than averaged weighted probability voting (ranked last). Finally, for the remaining 8\% of parameters leading to statistically significant results, we can see that weighted averaged probability voting (ranked first) takes less time than averaged weighted probability voting (ranked last).
This result, in combination with the results from the prediction accuracy, suggests that we should lean towards using either probability or weighted averaged probability voting due to the execution time improvements and insignificant prediction accuracy differences.

\section{Windowing type}
\subsection{Without drift detection}
\subsubsection{$\kappa_t$}
Almost all sets of parameters reject the null hypothesis with a p value below 0.05 indicating that not all window types result in equal prediction accuracy. A post-hoc Nemenyi test indicates that for a quarter of parameter sets that sliding outperforms hybrid windows with a p valueof 0.05.
Sliding windowing is always leads to more accurate predictions than hybrid windowing.

\subsubsection{Execution time}
The null hypothesis is rejected for all sets of parameters indicating that the windowing type has a significant effect on the execution time of a simulation. The Friedman test outputs a p value below 0.05. The post-hoc Nemenyi test supports roughly three quarters of the parameter sets with a p value below 0.05. Hybrid windowing is always faster than sliding windowing.

This experiment shows a trade-off between execution time and prediction accuracy. Therefore we recommend using sliding windows when prediction accuracy is more important in a specific use-case and to use hybrid windowing when execution time is more important.

\subsection{With drift detection}
Considering that tumbling windows were never the best nor worst for both execution time and prediction accuracy, we did not include them in the drift detection test to reduce the number of parameter combinations to test. This means that the Wilcoxon test was used, instead of the Friedman and post-hoc Nemenyi test because we are only comparing two parameter values.

\subsubsection{$\kappa_t$}
60\% of parameter combinations show a statistical difference that rejects the null hypothesis with a p-value of at most 0.05. 97\% of which show that sliding windows predicts more accurately than hybrid windows with statistical significance (sliding windows predict more accurately than hybrid windowing for 59\% of the non statistically significant results). This means, for the parameter configurations that led to statistical significant differences in the results (and in over half of those that were not significantly different), that sliding windows perform better. This is more or less expected as the algorithms can learn from an instance more than once. Hybrid windows are therefore almost always not the best choice when prediction accuracy is important.

\subsubsection{Execution time}
When looking at execution time, 97\% of parameter combinations show a statistical difference leading us to believe that the choice of windowing type has a drastic effect on the execution time of the learning algorithm chosen. Indeed, 99\% of these parameter combinations show that hybrid windowing leads to a smaller execution time than sliding windowing. Again, this is expected as the idea behind them is to reduce the amount of times a learning algorithm learns from a given instance. When using sliding windows, a learning algorithm should see a given instance as many times as the size of the window; however, when using hybrid windows, a learning algorithm should only see a given instance as many times as there are learning algorithms inside an ensemble (the classifiers in the ensemble each see the instance only once, but the ensemble technically sees the instance as many times as the number of classifiers it is comprised of).


\section{Drift Reset Type}
\subsection{With drift detection}
\subsubsection{$\kappa_t$}
Only 1 out of 330 parameter combinations show a statistical significance in the $\kappa_t$ values. The fact that less than 1\% of parameter combinations show a statistical significance leads us to believe that the drift reset type parameter does not have a significant impact on the prediction accuracy of our ensemble. However, for the one parameter configuration that showed statistically significant results between the drift reset types, it was for a parameter configuration only using 60\% ground truth. It showed that blind drift reset performed best, and statistically significantly better than partial drift resets.

\subsubsection{Execution time}
When looking at execution times, 20\% of parameter configurations showed statistically significant differences in the results. Leading us to believe that drift reset types has a small impact on the execution time as a parameter. We can see, across 52\% of these configurations, that partial resets takes the least amount of time, and is most of the time statistically significantly better than blind resets. Across 39\% of these configurations, a complete reset is the fastest option and is most of the time statistically significantly better than blind resets. Across only 10\% of these configurations, blind resets take the least amount of time, and perform statistically significantly better than complete or partial resets at about the same frequency.
These results are expected as partial resets classifiers the least and only when drifts are detected, complete should cause a few more classifier resets as by definition is causes all classifiers to be reset but only when drifts are detected. However, blind resets all of the classifiers at a somewhat regular interval, therefore requiring more frequent complete retraining of classifiers.

\section{Drift Detector Count}
\subsection{With drift detection}
\subsubsection{$\kappa_t$}
Only 7\% of parameter combinations show a statistical difference, 59\% of which show that "one for ensemble" predicts more accurately than "one per classifier". We find about the same ratio for the non significant results. This leads us to believe that the drift detector count parameter does not have a significant impact on the prediction accuracy of the ensemble, but that it is preferable to use "one for ensemble" as it usually outperforms "one per classifier".

\subsubsection{Execution time}
93\% of parameter combinations show a statistical significant difference in execution time, 99\% of which show that "one for ensemble" uses less execution time than "one per classifier". This result strongly suggests that the drift detector count parameter has a very significant impact on the execution time of the ensemble and that "one for ensemble" is the obvious choice to minimise execution time.

Overall, the results lead us to believe that is is preferable to use "one for ensemble" as a drift detector count parameter as it usually leads to better prediction accuracy while ensuring that we minimise execution time.

\section{Ground truth}
\subsection{With drift detection}
\subsubsection{$\kappa_t$}
Only 29\% of parameter combinations show a statistical difference, 100\% of which show that using 100\% ground truth predicts better than when using 60\% ground truth when fitting classifiers to the data. As expected, learning using 100\% ground truth is always the best option when prediction accuracy is the most important measure, and ranks first in prediction accuracy, while using 60\% ground truth always ranks last.
These results are exactly as expected, considering that as the ground truth used decreases, the prediction accuracy decreases also.

\subsubsection{Execution time}
The results are unexpected, and not particularly consistent. 60\% ground truth is often the fastest, then 100\% ground truth is sometimes the fastest, and 90\% ground truth is rarely fastest.
We cannot rely on a particular value of ground truth to lead to consistently faster execution times.