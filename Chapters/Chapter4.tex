% Chapter 4

\chapter{Experimental Design\label{chapter:experimental_design}} % Main chapter title
In this chapter, we will first cover the software and hardware specifications, briefly go over the framework that we extended, describe the various datasets used, explain the experimental setup and the evaluation metrics, and we will cover how we plan to compare the contributions listed in the previous chapter to the state of the art; or in the case of potential improvements, how they perform in regards to the original version of each respective algorithm.

%----------------------------------------------------------------------------------------

\section{Software and Hardware specifications}
In order to ensure that our experiments are reproducible, we will specify the specifications of the hardware and software used to run them. All experiments were run on a MacBook Pro model \textit{11,4}. This machine was running macOS 10.14.1 on a quad-core Intel i7 2.2GHz processor (\textbf{3.4GHz} TurboBoost, 8 virtual cores), with a 256 GB SSD, and \textbf{16GB of RAM}. During the course of the experiments, the laptop was plugged in and fully charged.

Python 3.7.1 was installed, with the following software packages:
\begin{itemize}
\item \textit{sortedcontainers} v2.0.5
\item \textit{numpy} v1.15.3
\item \textit{scipy} v1.1.0
\item \textit{scikit-learn} v0.20.0
\item \textit{pandas} v0.23.4
\item \textit{mlxtend} v0.13.0
\item \textit{scikit-multiflow} (latest commit from master rebased onto our branch) \textit{\textbf{\#f18a433}}
\end{itemize}

\section{Scikit-multiflow}
Scikit-multiflow \cite{skmultiflow} is a Python framework, that complements scikit-learn, an offline, batch learning Python library. As it currently stands, scikit-multiflow implements stream generators, machine learning algorithms, drift detection algorithms and evaluation methods for data streams. The authors describe scikit-multiflow as "an open-source framework for multi-output / multi-label and stream data mining", and takes its inspiration from MOA and MEKA. The former is the most popular data stream mining framework, implemented in Java, which includes a collection of machine learning algorithms, and evaluation tools. The latter is another project implemented in Java, but for multi-label learning and evaluation.

We selected this framework as it is backed by A. Bifet, implemented in Python: the most popular language among data scientists, open-source, and very recent.

\section{Data sets}
Four of the synthetic data sets used ($CIRCLES$, $SINE1$, $LED$, and $MIXED$) were generated by my colleague for his paper \cite{pesaranghader2016fast} where he proposed FHDDMS, the drift detection algorithm. These datasets were generated with ten percent (10\%) noise to test the robustness of his drift detection algorithm against noisy data streams. The data sets contain one hundred thousand rows belonging to one of two classes. These synthetic data sets can be traced back to \cite{10.1007/3-540-59286-5_74} and were further used in the following papers \cite{nishida2007detecting, gama2004learning, baena2006early}. % Given that these data sets are synthetic with drifts beginning at a known location, we can also determine the drift detection delay as well as true and false positive rates.

\subsection{$CIRCLES$}
As stated by Gama, Joao et al. in \cite{gama2004learning}, this data set is composed of two relevant numerical attributes: $x$ and $y$, which are uniformly distributed in [0, 1]. There are four different concepts in this data set, each representing whether or not a point is within a circle given $x$, and $y>$ coordinates for its center and its radius $r_c$. This data set contains gradual concept drifts that occur at every twenty-five thousand (25 000) instances. The four pairs of $<(x,y), r_c>$ defining each concept are given in table \ref{table:circle_concepts}.

\begin{table}[]
\centering
\caption{\label{table:circle_concepts}$CIRCLES$ data set concepts}
\begin{tabular}{|c|c|c|c|c|}
\hline
center & (0.2, 0.5) & (0.4, 0.5) & (0.6, 0.5) & (0.8, 0.5) \\ \hline
radius & 0.15       & 0.2        & 0.25       & 0.3        \\ \hline
\end{tabular}
\end{table}

\subsection{$SINE1$}
As stated by Gama, Joao et al. in \cite{gama2004learning}, this data set contains abrupt concept drifts, with noise-free examples. It has only two relevant numerical attributes, for which the values are uniformly distributed in [0, 1]. Before the concept drift, all instances for values below the curve $y = sin(x)$ are classified as \textbf{positive}. Then, after the concept drift, the rule is reversed; therefore the values below the curve become \textbf{negative}. The drifts were generated at every twenty thousand (20 000) instances.

\subsection{$MIXED$}
As stated by Gama, Joao et al. in \cite{gama2004learning}, this data set contains abrupt concept drifts and uses four relevant attributes. Two of which are boolean, let them be $v$ and $w$; and the other two attributes are numerical, in [0, 1]. Instances belong to the positive class if two of three conditions are met: $v$ is true, $w$ is true, $y < 0.5 + 0.3 \times sin(3\pi x)$. For each concept drift, the conditions are reversed, meaning that if the conditions are met, it'll be a positive instance, then after the drift, it will be a negative instance. The abrupt concept drifts occur at every twenty thousand (20 000) instances.

\subsection{$LED$}
The LED data set is a well-known synthetic data set originating from \cite{breiman1984classification}, which can be found at the UCI repository \cite{blake1999uci}. The data set simulates a seven-segment LED screen, on which a single digit is displayed and which we must predict. There are twenty-four binary attributes of which seventeen are irrelevant, and one numerical class attribute distributed in [0, 9]. In other words, there are ten concepts and seven relevant attributes that encode them. As the UCI repository states, without the introduction of noise, this problem would be trivial, and therefore each attribute value has a 10\% probability of having its value inverted. It is also noted that the optimal Bayes rate for this data set has a misclassification rate of 26\%, or a 74\% classification accuracy. The LED dataset contains gradual concept drifts occurring at every twenty-five thousand (25 000) instances.

\subsection{Streaming Ensemble Algorithm generator}
First described in \cite{street2001streaming} by Street and Kim, the Streaming Ensemble Algorithm (SEA) is used to generate streams with abrupt concept drift. It is composed of three numerical attributes of values in [0, 10], and only the first two attributes are relevant. For each instance, the class is determined by checking if the sum of the two relevant attributes passes a threshold value. Let $f_1$ and $f_2$ be the two numerical relevant attributes, and $\theta$ the threshold. An instance belongs to class one if $f_1 + f_2 <= \theta$. As in Street's paper, our stream has four concepts, with the threshold values for each being 8, 9, 7 and 9.5. We generated a stream of one hundred thousand instances, from zero to twenty percent noise, in five percent increments ($\{0; 5; 10; 15; 20\%\}$). Drifts, therefore, occur at every twenty-five thousand instances

\section{Estimation techniques}
In an offline setting, with static data, the most common evaluation method used is called cross-validation. However, given how little time a model has to learn from each instance due to the velocity of data streams and the risk of concept drift. It is for that reason that cross-validation is not suited for an online setting, but the two following techniques are.

\subsection{Holdout}
As the name implies, two distinct data sets are required for this technique: a training data set to train a model, and another to test it.  Cross-validation is typically used in offline static data mining, but is too computationally heavy and/or can be too time-consuming in a streaming setting, and therefore the validation step is skipped and performance is measured against a single holdout set.\cite{bifet2009data}
 
This technique is most useful when the training and testing sets have already been defined, as it makes comparing results from different studies possible.
In order to track performance, the model is evaluated periodically, but not too frequently to avoid negatively impacting performance.

The holdout data set can be sourced from new tuples arriving from the data stream, and can also be added later to the training set in order to optimize instance use.

It should be sufficient to safely use a single static holdout set if we make the assumption that there is no concept drift. Past studies consider test sets of over tens of thousands of instances to also be sufficient. 

\subsection{Interleaved test-then-train, or prequential}
Another evaluation method is the so-called prequential evaluation, also known as the interleaved test-then-train method. Again, as the name suggests, the evaluation method consists in first testing the classifier on a given set of instances, then immediately training the classifier on that same set. The evaluation strategy, therefore, ensures that the model has not previously seen testing tuples, and no holdout testing set is necessary, and the accuracy of the model is therefore incrementally updated. This method also allows us to use all of the data for both testing and training. Furthermore, as there is more testing data, each instance used to assess the accuracy and performance of the model weighs less than it would have in a smaller holdout test set.

For this thesis, all experiments will be done using the interleaved test-then-train evaluation technique.

\section{Performance measures\label{section:performance_measures}}


\subsection{Accuracy \& Confusion Matrix}
Let us assume that we are dealing with a binary classification problem (with two classes) with a \textbf{P}ositive and \textbf{N}egative class.

A confusion matrix keeps track of the veracity of our classifying model by using the following four measurements.

\begin{itemize}
\item False Positive (FP): number of instances incorrectly classified as positive
\item False Negative (FN): number of instances incorrectly classified as negative
\item True Positive (TP): number of instances correctly classified as positive
\item True Negative (TN): number of instances correctly classified as negative
\end{itemize}

It should then be clear that all of the positive class instances (P) are in the combined groups of FN and TP, and that all of the negative class instances (N) are in the remaining FP and TN groups.

A confusion matrix helps us visualize these measurements by presenting them in a matrix (two-by-two grid in the case of a binary classification problem). Each column represents all of the instances for an actual class, whereas each row represents the instances predicted for a given class. Table \ref{table:confusion_matrix} shows a confusion matrix.

\begin{table}[]
\centering
\caption{Confusion Matrix\label{table:confusion_matrix}}
\begin{tabular}{|c|c|c|}
\hline
                   & Actual Positive & Actual Negative \\ \hline
Predicted Positive & TP              & FN              \\ \hline
Predicted Negative & FP              & TN              \\ \hline
\end{tabular}
\end{table}

Given our four measurements above, we can calculate the following metrics:
\begin{itemize}
\item The \textit{sensitivity} and \textit{specificity} (or \textit{recall}) indicate the completeness for a given class of instances retrieved that belong to that class; in other words the percentage of correctly classified instances of a given class that were found overall.\begin{center}$\frac{TP}{TP+FN}=\frac{TP}{P}$ and $\frac{TN}{TN+FP}=\frac{TN}{N}$\end{center}
\item The \textit{accuracy}, while not a reliable metric, indicates the number of correct predictions over all cases to be predicted.\begin{center}$sensitivity\times\frac{P}{P+N}\times specificity\times\frac{N}{P+N}$ = $\frac{TP+TN}{P+N}$\end{center}
\item \textit{Precision} indicates the exactness of predictions for a given class. In other words, the percentage of instances the classifier predicted as positive that were actually positive.\begin{center}$\frac{TP}{TP+FP}$\end{center}
\item \textit{F-measure}, which is the harmonic mean of precision and recall. \begin{center}$2\times\frac{precision\times recall}{precision+recall}$\end{center}
\end{itemize}

\subsection{Kappa ($\kappa$) statistics}
\subsubsection{$\kappa$ statistic}
Bifet and Frank argue in \cite{bifet2010sentiment} that prequential accuracy is not well suited for classifying unbalanced data in a stream whereas the Kappa ($\kappa$) statistic proposed by Cohen in \cite{cohen1960coefficient} is better adapted when dealing with changing class distributions. They therefore proposed a sliding-window kappa as defined by equation \ref{eq:kappa}
\begin{equation}
\kappa=\frac{p_0-p_c}{1-p_c}
\label{eq:kappa}
\end{equation}where $p_0$ is the classifier's prequential accuracy and $p_c$ is the probability of a chance (or no-change) classifier makes a correct prediction. Refer to \cite{bifet2010sentiment} for the equations used to calculate $p_0$ and $p_c$.

Values of $\kappa$ are in [0, 1]. If the classifier always classifies instances correctly, then $p_0=1$ and $\kappa=1$; if the classifier correctly classifies instances as often as the chance classifier, then $p_0 = p_c$ and $\kappa=0$; and if the chance classifier is always right, then $p_c=1$ and $\kappa=1$.

\subsubsection{$\kappa^+$ statistic}
The $\kappa^+$ statistic is an improvement upon the regular $\kappa$ statistic that takes into account temporal dependence in order better evaluation the performance of a given classifier. When there is no temporal dependency in the data and that classes are balanced, then $\kappa^+$ is equal to $\kappa$. While we could solely rely on $\kappa^+$, the authors however recommend using both $\kappa$ and $\kappa^+$ statistics in order to obtain a more thorough evaluation.
$\kappa^+$ is defined in equation \ref{eq:kappa_plus}
\begin{equation}
\label{eq:kappa_plus}
\kappa^+=\frac{p_0-p'_e}{1-p'_e}
\end{equation}where $p_0$ is the classifier's prequential accuracy and $p'_e$ is the accuracy of a no-change classifier. The no-change classifier predicts that the next class will be the same as the last seen class label. Just like the original $\kappa$ statistic, $\kappa^+\in [0, 1]$. Refer to \cite{DBLP:conf/pkdd/2013-1} for the equations relating to calculating $p_0$ and $p'_e$. $\kappa^+$ is also sometimes referred to as $\kappa$-temporal \cite{vzliobaite2015evaluation}.

\subsubsection{$\kappa_m$ statistic}
The $\kappa_m$ statistic is another improvement upon the regular to indicate whether a classifier is performing better than a majority-class classifier \cite{bifet2015efficient}. Its definition is given in equation \ref{eq:kappa_m}.

\begin{equation}
    \label{eq:kappa_m}
    \kappa_m = \frac{p_0-p_m}{1-p_m}
\end{equation}where $p_0$ is the classifier's prequential accuracy and $p_m$ is the prequential accuracy of a majority-class classifier. If the classifier always predicts correctly, then $\kappa_m=1$. If the classifier predicts correctly as often as the majority-class one, then $\kappa_m=0$.

\subsection{Testing for Statistical Significance}
While the measures presented in \ref{section:performance_measures} are useful for evaluating the performance of our classifiers, it is insufficient to rely solely on them to fully evaluate performance differences between classifiers. Statistical significance tests are used to determine whether or not the differences that were observed are statistically significant and not due to simple coincidence \cite{japkowicz2011evaluating}.

Popular choices for statistical tests in the machine learning community for comparing multiple classifiers across multiple cases are the Analysis of Variance (ANOVA) and the Friedman test. 

The ANOVA test assumes a normal data distribution whereas the Friedman test does not. It is for this reason, coupled with the fact that the Friedman test is also non-parametric, that we have chosen the latter as our choice for a statistical significance test. 

\subsubsection{The Friedman test}
The test ranks each algorithm separately for each data set and computes a test statistic using the variation within the ranks and the variation within the error variation. The Friedman statistic $\chi^2_F$ is defined in equation \ref{eq:friedman_statistic}, taken from \cite{japkowicz2011evaluating};

\begin{equation}
\label{eq:friedman_statistic}
\chi^2_F = \frac{SS_{Total}}{SS_{Error}}=\bigg[\frac{12}{n\times k\times(k+1)}\times\sum_{j=1}^k(R_.j)^2\bigg]-3\times n \times (k+1)
\end{equation}where $n$ is the number of data sets, $k$ is the number of algorithms considered, and $R$ simply denotes ranking. And finally $(k-1)$ is known as the degrees of freedom.

The Friedman statistic ($\chi^2_F$) is then looked up in the $\chi^2$ distribution table to obtain a \textit{probability value} (p-value), signifying $P(\chi^2_{k-1} \geq \chi^2_F )$. This p-value is widely used in null-hypothesis testing by measuring it against a threshold value called a significance level. A lower p-value means a higher statistical significance. Traditionally, the significance level is either 1\% or 5\%, and denoted as $\alpha$. The null hypothesis is accepted if the p-value is greater than $\alpha$, otherwise, $H_1$ is accepted and the null hypothesis is rejected.

When the null hypothesis is rejected following a Friedman test, post hoc Nemenyi tests are recommended by Japkowicz and Shah in \cite{japkowicz2011evaluating}.

\subsubsection{Post-hoc test: the Nemenyi test}
Nemenyi tests are used in order to determine if the rankings obtained from the Friedman test are statistically significant. It computes a statistic, $q$, as defined in equation \ref{eq:nemenyi_q} for a pair of classifiers $f_{j1}$ and $f_{j2}$. $q$ gives us the difference between ranks of the two algorithms, and can be also expressed as a "critical distance". In order compare the rankings, the Nemenyi test computes the mean rank for a given classifier using equation \ref{eq:nemenyi_mean_rank} where $R_{ij}$ is the rank of classifier $f_j$ on data set $S_i$.
\begin{equation}
\label{eq:nemenyi_q}
\overline{R}_{.j}=\frac{1}{n}\sum_{i=1}^nR_{ij}
\end{equation}\begin{equation}
\label{eq:nemenyi_mean_rank}
q=\frac{\overline{R}_{.j1}-\overline{R}_{.j2}}{\sqrt{\frac{k(k+1)}{6n}}}
\end{equation}

\section{Experiments}
For these experiments, the following classifiers were used: Gaussian Naive Bayes (G\_NB), Leverage Bagging, Stochastic Gradient Descent (SGD), Multinomial Naive Bayes (M\_NB), and our Voting Ensemble composed of three sub-classifiers. The three classifiers inside the voting ensemble are the SGD, M\_NB, and G\_NB. Finally, we also used a no-change classifier as well as a majority-class classifier as our baselines.

Readers familiar with classification algorithms might know that some advantages of NB are its scalability, high accuracy, ability to use prior knowledge, and the fact that it has comparable performance to decision trees and neural networks. NB is incrementally more confident, and easy to implement. However, the downsides are its significant compute costs, and that using conditional dependencies reduces accuracy due to the real dependency between attributes.
SGDs have generally high predictive accuracy, generalize well meaning that they are robust and good for noisy domains, and good for incremental learning. However, they are known to have a slow training time, may fail to converge, and output a black box model which means that results cannot be interpreted.
Leverage bagging \cite{bifet2010leveraging} is an improvement over the Online Bagging technique of Oza and Russel. They claim to introduce more randomization to the online bagging algorithm to achieve a more accurate classifier. However, the authors noted that subagging, half subagging, and bagging with-out replacement ran faster but were marginally less accurate.

Unless stated otherwise, our default parameters for the experiments are listed in table \ref{table:default_exp_parameters} and the default parameters for the classifiers are listed in table \ref{table:default_clf_parameters}. These values were obtained through extensive experimentation.

Finally, each experiment is run on five different examples each sourced from four synthetic data sets (5 examples of $SINE1$, another 5 of $CIRCLES$, etc.) and five streams generated by the SEA generator with levels of noise in increments of 5\% (from 0\% to 20\%), for a grand total of twenty-five (25) streams of one hundred thousand (100000) instances.

\begin{table}[]
\centering
\caption{\label{table:default_exp_parameters}Default parameters for the experiments}
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Parameter}} & \multicolumn{1}{c|}{\textbf{Value}} \\ \hline
Classifier & Ensemble Voting Classifier \\ \hline
Pretrain size & 1 000 \\ \hline
Max samples & 100 000 \\ \hline
Batch size & 33 \\ \hline
Window size & 99 \\ \hline
Window type & sliding tumbling hybrid \\ \hline
Ground truth \% & 100 \\ \hline
Drift reset type & none \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{\label{table:default_clf_parameters}Our default parameters for all classifiers}
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Classifier}} & \multicolumn{1}{c|}{\textbf{Parameters}} \\ \hline
Leverage Bagging & \begin{tabular}[c]{@{}l@{}}base\_estimator=KNN()\\ n\_estimators=2\\ w=6\\ delta=0.002\\ leverage\_algorithm=leveraging\_bag\\ random\_state=None\end{tabular} \\ \hline
Gaussian NB & \begin{tabular}[c]{@{}l@{}}priors=None\\ var\_smoothing=1e-9\end{tabular} \\ \hline
Multinomial NB & \begin{tabular}[c]{@{}l@{}}alpha=1.0\\ fit\_prior=True\\ class\_prior=None\end{tabular} \\ \hline
Stochastic Gradient Descent & \begin{tabular}[c]{@{}l@{}}loss=log\\ penalty=l2\\ alpha=0.0001\\ l1\_ratio=0.15\\ fit\_intercept=True\\ max\_iter=1000\\ tol=1e-3\\ shuffle=True\\ epsilon=0.1\\ n\_jobs=-1\\ random\_state=None\\ learning\_rate=optimal\\ eta0=0.0\\ power\_t=0.5\\ early\_stopping=False\\ validation\_fraction=0.1\\ n\_iter\_no\_change=5\\ average=False\\ n\_iter=None\end{tabular} \\ \hline
\end{tabular}
\end{table}

\subsection{Voting ensemble window types}
For this experiment, we wanted to determine how our proposed sliding tumbling windows perform against regular sliding and regular tumbling windows (with a $window\ size=33$, and not $99$).

In our experimentation, we are comparing the same classifier each time with a different window type ($k=3$), on twenty-three different data sets ($n=23$). Therefore the Friedman will test for the following hypothesis:
\begin{itemize}
\item $H_0$: The three window types will perform identically;
\item $H_1$: At least one pair of window types perform differently.
\end{itemize}

\subsection{Voting ensembles window and chunk sizes}
For this experiment, we wanted to determine how window and chunk sizes affected the performance of our voting ensemble over various voting types, window types and ground truth percentages. We, therefore, ran the experiments for the voting ensemble classifier with varying window and chunk sizes. $Batch\ size \in [5, 10, 25, 50, 75, 100]$ and $window\ size = 3\times batch\ size$ (because our ensemble comprises of 3 classifiers).
The purpose of this experiment is to determine if there are statistically significant differences in the prediction accuracy and/or execution time depending on the chunk size used for training the ensemble.

In our experimentation, we are comparing 6 chunk/window sizes ($k=6$), on eighteen different data sets ($n=18$). Therefore the Friedman will test for the following null hypothesis. NB: performance measures are $\kappa_t$ and execution time.
\begin{itemize}
\item $H_0$: The six chunk/window sizes will perform identically;
\item $H_1$: At least one pair of chunk/window sizes perform differently.
\end{itemize}

\subsection{Voting ensemble voting strategies}
For this experiment, we wanted to determine how the voting strategy affected the performance of our voting ensemble while using sliding tumbling windows. We, therefore, ran the experiments using both of the provided voting strategies, being the majority voting ("hard") and the highest mean probability ("soft"), as well as our new strategy proposed in section \ref{section:new_voting_strategy}.
We will, of course, compare these results with those of the leverage bagging, no-change, and majority-class classifiers.

\subsection{Relationship between drift reset strategies \& ground truth}
For this experiment, we wanted to determine how the drift reset strategy and the percentage of ground truth used for training affected the performance of our voting ensemble while using sliding tumbling windows. We therefore ran experiments for the classifier with five different drift reset strategies (complete reset, no reset, partial reset where each classifier in the ensemble has a 70\% chance of being reset, blind reset at random regular intervals:  $r\times batchSize, r\in[1,7]$), each while using varying amounts of ground truth for the training: $ground\ truth \in \{100, 90, 80, 70, 60, 50\}\%$.
We will, of course,  compare these results with those of the leverage bagging, no-change, and majority-class classifiers.

\subsection{Evaluating drift detection over various window types}
For this experiment, we wanted to determine how the drift detection performed depending on the window type. Recall that these are the hybrid sliding tumbling, sliding, or tumbling windows (tumbling windows require a window size of 33), while using the partial drift reset strategy and 75\% of ground truth for the training of our voting ensemble.
We will, of course, be comparing these results with those of the leverage bagging, no-change and majority-class classifiers (while using 100\% ground truth). The leverage bagging classifier is implemented with a built-in ADWIN drift detector.

\subsection{Comparing all classifiers}
For this experiment, we wanted to examine how our Voting Ensemble performed against all of the other classifiers listed above while using our Sliding Tumbling windows.
In order to do so, we set up each classifier to run with the default parameters listed in tables \ref{table:default_exp_parameters} and \ref{table:default_clf_parameters}.

In our experimentation, we are comparing seven classifiers ($k=7$), on twenty-five different data sets ($n=25$). Therefore the Friedman will test for the following hypothesis:
\begin{itemize}
\item $H_0$: The six classifiers will perform identically;
\item $H_1$: At least one pair of classifiers perform differently.
\end{itemize}


\section{Summary}
In this chapter, we described the experimental setup, the data sets used for our analysis in the upcoming chapter, and the estimation techniques. Using the interleaved test-then-train evaluation technique, coupled with the performance measures listed in section \ref{section:performance_measures}. The Friedman test will be used to determine if our findings are statistically significant, and confirmed by using post hoc Nemenyi tests.

In the following chapter, we will present the results of our experiments comparing our contributions to the baseline and state-of-the-art algorithms, giving our analysis of these findings and of their significance.

% through extensive experimentation or inspection [wording!!!]