% Chapter 4

\chapter{Experimental Design} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter3} 
In this chapter we will cover how the contributions listed in the previous chapter compare to the state of the art; or in the case of potential improvements, how they perform in regards to the original version of each respective algorithm. In order to do so, we will first cover the various data sets and the software and hardware used. Next, we will cover the actual experiments conducted and wrap the chapter up with a discussion.

%----------------------------------------------------------------------------------------

\section{Experimental Design}

\section{Software and Hardware specifications}
In order to ensure that our experiments are reproducible, we will specify the specifications of the hardware and software used to run them. All experiments were run on a MacBook Pro 11,4. This machine was running macOS 10.14.1 on a quad-core Intel i7 2.2GHz processor (\textbf{3.4GHz} TurboBoost, 8 virtual cores), and \textbf{16GB of RAM}. During the course of the experiments, the laptop was plugged in and fully charged.

Python 3.7.0 was installed, with the following software packages:
\begin{itemize}
\item \textit{sortedcontainers} v2.0.5
\item \textit{numpy} v1.15.3
\item \textit{scipy} v1.1.0
\item \textit{scikit-learn} v0.20.0
\item \textit{pandas} v0.23.4
\item \textit{mlxtend} v0.13.0
\item \textit{scikit-multiflow} (latest commit from master rebased onto our branch) \textit{\textbf{\#f18a433}}
\end{itemize}

\section{Scikit-multiflow}
Scikit-multiflow \cite{skmultiflow} is a Python framework, that complements scikit-learn, an offline, batch learning Python library. As it currently stands, scikit-multiflow implements stream generators, machine learning algorithms, drift detection algorithms and evaluation methods for data streams. The authors describe scikit-multiflow as "an open-source framework for multi-output / multi-label and stream data mining", and takes its inspiration from MOA and MEKA. The former is the most popular data stream mining framework, implemented in Java, which includes a collection of machine learning algorithms, and evaluation tools. The latter is another project implemented in Java, but for multi-label learning and evaluation.

We selected this framework as it is backed by A. Bifet, implemented in Python and not Java, open-source, and very recent.

\section{Data sets}
Four of the synthetic data sets used ($CIRCLES$, $SINE1$, $LED$, and $MIXED$) were generated by my colleague for his paper \cite{pesaranghader2016fast} where he proposed FHDDMS, the drift detection algorithm. These data sets were generated with ten percent (10\%) noise to test the robustness of his drift detection algorithm against noisy data streams. The data sets contain one hundred thousand rows belonging to one of two classes. These synthetic data sets can be traced back to \cite{10.1007/3-540-59286-5_74} and were further used in the following papers \cite{nishida2007detecting, gama2004learning, baena2006early}. % Given that these data sets are synthetic with drifts beginning at a known location, we can also determine the drift detection delay as well as true and false positive rates.

\subsection{$CIRCLES$}
As stated by Gama, Joao et al. in \cite{gama2004learning}, this data set is composed of two relevant numerical attributes: $x$ and $y$, which are uniformly distributed in [0, 1]. There are four different concepts in this data set, each representing whether or not a point is within a circle given $x$, and $y>$ coordinates for its center and its radius $r_c$. This data set contains gradual concept drifts that occur at every twenty five thousand (25 000) instances. The four pairs of $<(x,y), r_c>$ defining each concept are given in table \ref{table:circle_concepts}.

\begin{table}[]
\centering
\caption{\label{table:circle_concepts}$CIRCLES$ data set concepts}
\begin{tabular}{|c|c|c|c|c|}
\hline
center & (0.2, 0.5) & (0.4, 0.5) & (0.6, 0.5) & (0.8, 0.5) \\ \hline
radius & 0.15       & 0.2        & 0.25       & 0.3        \\ \hline
\end{tabular}
\end{table}

\subsection{$SINE1$}
As stated by Gama, Joao et al. in \cite{gama2004learning}, this data set contains abrupt concept drifts, with noise-free examples. It has only two relevant numerical attributes, for which the values are uniformly distributed in [0, 1]. Before the concept drift, all instances for values below the curve $y = sin(x)$ are classified as \textbf{positive}. Then, after the concept drift, the rule is reversed ; therefore the values below the curve become \textbf{negative}. The drifts were generated at every twenty thousand (20 000) instances.

\subsection{$MIXED$}
As stated by Gama, Joao et al. in \cite{gama2004learning}, this data set contains abrupt concept drifts, and uses four relevant attributes. Two of which are boolean, let them be $v$ and $w$; and the other two attributes are numerical, in [0, 1]. Instances belong to the positive class if two of three conditions are met: $v$ is true, $w$ is true, $y < 0.5 + 0.3 * sin(3\pi x)$. For each concept drift, the conditions are reversed, meaning that if the conditions are met, it'll be a positive instance, then after the drift it will be a negative instance.

\subsection{$LED$}
The LED data set is a well-known synthetic data set  originating from \cite{breiman1984classification}, which can be found at the UCI repository \cite{blake1999uci}. The data set simulates a seven-segment LED screen, on which a single digit is displayed and which we must predict. There are twenty four binary attributes of which seventeen are irrelevant, and one numerical class attribute distributed in [0, 9]. In other words, there are ten concepts and seven relevant attributes that encode them. As the UCI repository states, without the introduction of noise, this problem would be trivial, and therefore a each attribute value has a 10\% probability of having its value inverted. It is also noted that the optimal Bayes rate for this data set has a misclassification rate of 26\%, or a 74\% classification accuracy.

\subsection{Streaming Ensemble Algorithm generator}
First described in \cite{street2001streaming} by Street and Kim, the Streaming Ensemble Algorithm (SEA) is used to generate streams with abrupt concept drift. It is composed of three numerical attributes of values in [0, 10], and only the first two attributes are relevant. For each instance, the class is determined by checking if the sum of the two relevant attributes passes a threshold value. Let $f_1$ and $f_2$ be the two numerical relevant attributes, and $\theta$ the threshold. An instance belongs to class one if $f_1 + f_2 <= \theta$. Like Street, our stream has four concepts, with the threshold values for each being 8, 9, 7 and 9.5. Drifts therefore occur at every twenty five thousand instances. We generated a stream of one hundred thousand instances, from zero to twenty percent noise, in five percent increments ($\{0; 5; 10; 15; 20\%\}$).

\section{Estimation techniques}
In an offline setting, with static data, the most common evaluation method used is called cross-validation. However, given how little time a model has to learn from each instance due to the velocity of data streams and the risk of concept drift. It is for that reason that cross-validation is not suited for an online setting, but the two following techniques are.

\subsection{Holdout}
As the name implies, two distinct data sets are required for this technique: a training data set to train a model, and another to test it.  Cross validation is typically used in offline static data mining, but is too computationally heavy and/or can be too time consuming in a streaming setting, and therefore the validation step is skipped and performance is measured against a single holdout set.\cite{bifet2009data}
 
This technique is most useful when the training and testing sets have already been defined, as it makes comparing results from different studies possible.
In order to track performance, the model is evaluated periodically, but not too frequently to avoid negatively impacting performance.

The holdout data set can be sourced from new tuples arriving from the data stream, and can also be added later to the training set in order to optimize instance use.

It should be sufficient to safely use a single static holdout set if we make the assumption that there is no concept drift. Past studies consider test sets of over tens of thousands of instances to also be sufficient. 
\subsection{Interleaved test-then-train, or prequential}
Another evaluation method is the so-called prequential evaluation, also known as the interleaved test-then-train method. Again, as the name suggests, the evaluation method consists in first testing the classifier on a given set of instances, then immediately training the classifier on that same set. The evaluation strategy therefore ensures that the model has not previously seen testing tuples, and no holdout testing set is necessary; and the accuracy of the model is therefore incrementally updated. This method also allows us to use all of the data for both testing and training. Furthermore, as there is more testing data, each instance used to assess the accuracy and performance of the model weighs less than it would have in a smaller holdout test set.

\section{Performance measures}


\subsection{Accuracy \& Confusion Matrix}
Let us assume that we are dealing with a binary classification problem (with two classes) with a \textbf{P}ositive and \textbf{N}egative class.

A confusion matrix keeps track of the veracity of our classifying model by using the following four measurements.

\begin{itemize}
\item False Positive (FP): number of instances incorrectly classified as positive
\item False Negative (FN): number of instances incorrectly classified as negative
\item True Positive (TP): number of instances correctly classified as positive
\item True Negative (TN): number of instances correctly classified as negative
\end{itemize}

It should then be clear that all of the positive class instances (P) are in the combined groups of FN and TP, and that all of the negative class instances (N) are in the remaining FP and TN groups.

A confusion matrix helps us visualize these measurements by presenting them in a matrix (two-by-two grid in the case of a binary classification problem). Each column represents all of the instances for an actual class, whereas each row represents the instances predicted for a given class. Table \ref{table:confusion_matrix} shows a confusion matrix.

\begin{table}[]
\centering
\caption{Confusion Matrix\label{table:confusion_matrix}}
\begin{tabular}{|c|c|c|}
\hline
                   & Actual Positive & Actual Negative \\ \hline
Predicted Positive & TP              & FN              \\ \hline
Predicted Negative & FP              & TN              \\ \hline
\end{tabular}
\end{table}

Given our four measurements above, we can calculate the following metrics:
\begin{itemize}
\item The \textit{sensitivity} and \textit{specificity} indicate the rate of TP and TN respectively\begin{center}$\frac{TP}{P}$ and $\frac{TN}{N}$\end{center}
\item The \textit{accuracy}, while not a reliable metric, indicates the number of correct predictions over all cases to be predicted.\begin{center}$sensitivity*\frac{P}{P+N}*specificity*\frac{N}{P+N}$ = $\frac{TP+TN}{P+N}$\end{center}
\item \textit{Precision} indicates the exactness of predictions for a given class. In other words, the percentage of instances the classifier predicted as positive that were actually positive.\begin{center}$\frac{TP}{TP+FP}$\end{center}
\item \textit{Recall} indicates the completeness for a given class of instances retrieved that belong to that class; in other words the percentage of correctly classified instances of a given class that were found overall.\begin{center}$\frac{TP}{TP+FN}$\end{center}
\item \textit{F-measure}, which is the harmonic mean of precision and recall. \begin{center}$2*\frac{precision*recall}{precision+recall}$\end{center}
\end{itemize}

\subsection{Kappa statistics}


\subsection{Tests for Statistical Significance}
(not ANOVA, but Friedman because of normal distribution assumption)
Post nemenyi
Friedman test

Richards thesis or paper -> graph

\section{Experiments [ultra draft mode :(]}
For these experiments, the following classifiers were used: Gaussian Naive Bayes (G\_NB), Leverage Bagging, Stochastic Gradient Descent (SGD), Multinomial Naive Bayes (M\_NB), and our Voting Ensemble composed of three sub-classifiers. The three classifiers inside the voting ensemble are the SGD, M\_NB and G\_NB.

Readers familiar with classification algorithms might know that some advantages of NB are its scalability, high accuracy, ability to use prior knowledge, and the fact that it has comparable performance to decision trees and neural networks. NB is incrementally more confident, and easy to implement. However, the downsides are its significant compute costs, and that using conditional dependencies reduces accuracy due to the real dependency between attributes.
SGDs have generally high predictive accuracy, generalize well meaning that they are robust and good for noisy domains, and good for incremental learning. However, they are known to have a slow training time, may fail to converge, and output a black box model which means that results cannot be interpreted.
Leverage bagging \cite{bifet2010leveraging} is an improvement over the Online Bagging technique of Oza and Russel. They claim to introduce more randomization to the online bagging algorithm to achieve a more accurate classifier. However, the authors noted that subagging, half subagging, and bagging with-out replacement ran faster but were marginally less accurate.

\subsection{Comparing all classifiers}
For this experiment, we wanted to determine how our Voting Ensemble with our Sliding Tumbling windows performed against all of the other classifiers listed above.

\subsection{Voting ensemble window types}
For this experiment, we wanted to determine how our proposed sliding tumbling windows perform against regular sliding and regular tumbling windows.

\subsection{Voting ensembles window and chunk sizes}
For this experiment, we wanted to determine how window and chunk sizes affected the performance of our voting ensemble. 

\subsection{Voting ensemble voting strategies}


\subsection{Relationship between drift reset strategies \& ground truth}


\subsection{Evaluating drift detection over various window types}


\section{Discussion}

\section{Summary}


% through extensive experimentation or inspection [wording!!!]