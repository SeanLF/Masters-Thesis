% Chapter 4

\chapter{Experimental Design} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter3} 
In this chapter we will cover how the contributions listed in the previous chapter compare to the state of the art; or in the case of potential improvements, how they perform in regards to the original version of each respective algorithm. In order to do so, we will first cover the various data sets and the software and hardware used. Next, we will cover the actual experiments conducted and wrap the chapter up with a discussion.

%----------------------------------------------------------------------------------------

\section{Experimental Design}

\section{Software and Hardware specifications}
In order to ensure that our experiments are reproducible, we will specify the specifications of the hardware and software used to run them. All experiments were run on a MacBook Pro 11,4. This machine was running macOS 10.14.1 on a quad-core Intel i7 2.2GHz processor (\textbf{3.4GHz} TurboBoost, 8 virtual cores), and \textbf{16GB of RAM}. During the course of the experiments, the laptop was plugged in and fully charged.

Python 3.7.0 was installed, with the following software packages:
\begin{itemize}
\item \textit{sortedcontainers} v2.0.5
\item \textit{numpy} v1.15.3
\item \textit{scipy} v1.1.0
\item \textit{scikit-learn} v0.20.0
\item \textit{pandas} v0.23.4
\item \textit{mlxtend} v0.13.0
\item \textit{scikit-multiflow} (latest commit from master rebased onto our branch) \textit{\textbf{\#f18a433}}
\end{itemize}

\section{Scikit-multiflow}
\cite{skmultiflow}

\section{Data sets}
Four of the synthetic data sets used ($CIRCLES$, $SINE1$, $LED$, and $MIXED$) were generated by my colleague for his paper \cite{pesaranghader2016fast} where he proposed FHDDMS, the drift detection algorithm. These data sets were generated with ten percent (10\%) noise to test the robustness of his drift detection algorithm against noisy data streams. The data sets contain one hundred thousand rows belonging to one of two classes. These synthetic data sets can be traced back to \cite{10.1007/3-540-59286-5_74} and were further used in the following papers \cite{nishida2007detecting, gama2004learning, baena2006early}. Given that these data sets are synthetic with drifts beginning at a known location, we can also determine the drift detection delay as well as true and false positive rates.

\subsection{$CIRCLES$}
As stated by Gama, Joao et al. in \cite{gama2004learning}, this data set is composed of two relevant numerical attributes: $x$ and $y$, which are uniformly distributed in [0, 1]. There are four different concepts in this data set, each representing whether or not a point is within a circle given $x$, and $y>$ coordinates for its center and its radius $r_c$. This data set contains gradual concept drifts that occur at every twenty five thousand (25 000) instances. The four pairs of $<(x,y), r_c>$ defining each concept are given in table \ref{table:circle_concepts}.

\begin{table}[]
\centering
\caption{\label{table:circle_concepts}$CIRCLES$ data set concepts}
\begin{tabular}{|c|c|c|c|c|}
\hline
center & (0.2, 0.5) & (0.4, 0.5) & (0.6, 0.5) & (0.8, 0.5) \\ \hline
radius & 0.15       & 0.2        & 0.25       & 0.3        \\ \hline
\end{tabular}
\end{table}

\subsection{$SINE1$}
As stated by Gama, Joao et al. in \cite{gama2004learning}, this data set contains abrupt concept drifts, with noise-free examples. It has only two relevant numerical attributes, for which the values are uniformly distributed in [0, 1]. Before the concept drift, all instances for values below the curve $y = sin(x)$ are classified as \textbf{positive}. Then, after the concept drift, the rule is reversed ; therefore the values below the curve become \textbf{negative}. The drifts were generated at every twenty thousand (20 000) instances.

\subsection{$MIXED$}
As stated by Gama, Joao et al. in \cite{gama2004learning}, this data set contains abrupt concept drifts, and uses four relevant attributes. Two of which are boolean, let them be $v$ and $w$; and the other two attributes are numerical, in [0, 1]. Instances belong to the positive class if two of three conditions are met: $v$ is true, $w$ is true, $y < 0.5 + 0.3 * sin(3\pi x)$. For each concept drift, the conditions are reversed, meaning that if the conditions are met, it'll be a positive instance, then after the drift it will be a negative instance.

\subsection{$LED$}
% insert description of data set
\subsection{Streaming Ensemble Algorithm generator}
First described in \cite{street2001streaming} by Street and Kim, the Streaming Ensemble Algorithm (SEA) is used to generate streams with abrupt concept drift. It is composed of three numerical attributes of values in [0, 10], and only the first two attributes are relevant. For each instance, the class is determined by checking if the sum of the two relevant attributes passes a threshold value. Let $f_1$ and $f_2$ be the two numerical relevant attributes, and $\theta$ the threshold. An instance belongs to class one if $f_1 + f_2 <= \theta$. Like Street, our stream has four concepts, with the threshold values for each being 8, 9, 7 and 9.5. Drifts therefore occur at every twenty five thousand instances. We generated a stream of one hundred thousand instances, from zero to twenty percent noise, in five percent increments ($\{0; 5; 10; 15; 20\%\}$).

\section{Evaluation methods}
\subsection{Holdout}
\subsection{Prequential: interleaved test-then-train}

\section{Performance measures}
\subsection{Accuracy \& Confusion Matrix}
(confusion matrix, tp, fp, accuracy)
\subsection{Kappa \& variations}
(regular and temporal)
\subsection{Tests for Statistical Significance}
(not ANOVA, but Friedman because of normal distribution assumption)

\section{Experiments [ultra draft mode :(]}
For these experiments, the following classifiers were used: Gaussian Naive Bayes (G\_NB), Leverage Bagging, Stochastic Gradient Descent (SGD), Multinomial Naive Bayes (M\_NB), and our Voting Ensemble composed of three sub-classifiers. The three classifiers inside the voting ensemble are the SGD, M\_NB and G\_NB.

Readers familiar with classification algorithms might know that some advantages of NB are its scalability, high accuracy, ability to use prior knowledge, and the fact that it has comparable performance to decision trees and neural networks. NB is incrementally more confident, and easy to implement. However, the downsides are its significant compute costs, and that using conditional dependencies reduces accuracy due to the real dependency between attributes.
SGDs have generally high predictive accuracy, generalize well meaning that they are robust and good for noisy domains, and good for incremental learning. However, they are known to have a slow training time, may fail to converge, and output a black box model which means that results cannot be interpreted.
Leverage bagging \cite{bifet2010leveraging} is an improvement over the Online Bagging technique of Oza and Russel. They claim to introduce more randomization to the online bagging algorithm to achieve a more accurate classifier. However, the authors noted that subagging, half subagging, and bagging with-out replacement ran faster but were marginally less accurate.

\subsection{Comparing all classifiers}
For this experiment, we wanted to determine how our Voting Ensemble with our Sliding Tumbling windows performed against all of the other classifiers listed above.

\subsection{Voting ensemble window types}
For this experiment, we wanted to determine how our proposed sliding tumbling windows perform against regular sliding and regular tumbling windows.

\subsection{Voting ensembles window and chunk sizes}
For this experiment, we wanted to determine how window and chunk sizes affected the performance of our voting ensemble. 

\subsection{Voting ensemble voting strategies}


\subsection{Relationship between drift reset strategies \& ground truth}


\subsection{Evaluating drift detection over various window types}


\section{Discussion}

\section{Summary}


% through extensive experimentation or inspection [wording!!!]